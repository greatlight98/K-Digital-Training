{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weight initialization\n",
    " - 모델에 곱해지는 웨이트의 초기값을 잘 설정해야 한다!\n",
    " - 웨이트를 초기화 하고 학습한 경우 학습이 더 잘 되고, 성능이 뛰어났다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weight 초기화 방법\n",
    " - 상수로 초기화하기 (0 제외! BP 할 때 weight가 0이면 모든 gradient가 0으로 바뀜)\n",
    " - Restricted Boltzmann Machine(RBM)\n",
    " - Deep Belief Network(DBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Restricted Boltzmann Machine(RBM)\n",
    " - Restricted : 한 레이어 안의 노드들끼리는 연결이 없다. 다만 A 레이어의 모든 노드와 B 레이어의 모든 노드가 Fully-connecte하게 연결 되어 있다.\n",
    " - RBM은 입력 X를 출력 Y로 진행하고(encoding), Y를 통해 X를 복원할 수 있다.(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. DBN과 Weight 초기화\n",
    "  - 요즘에는 사용되지 않는 방법이다! 구식 버전\n",
    " ### (1) Pre-training 과정 : Weight 초기화 과정\n",
    " - (1) RBM for x :  x 레이어와 hidden1 레이어에 RBM을 통해 입력 X를 출력 Y로 진행하는 encoding, Y를 통해 X를 복원하는 decoding을 학습한다.\n",
    " - (2) RBM for h^1 : hidden2 레이어를 더 쌓고, x 레이어와 hidden1 레이어의 weight를 고정시켜 놓는다. hidden1 레이어와 hidden2 레이어 사이에 RBM 학습을 한다.\n",
    " - (3) RBM for h^2 : hidden3 레이어를 더 쌓고, hidden1 레이어와 hidden2 레이어의 weight를 고정시켜 놓는다. hidden2 레이어와 hidden3 레이어 사이에 RBM 학습을 한다.\n",
    " - (4) 위 과정으로 출력 레이어까지 n개의 레이어 쌓음!\n",
    "  \n",
    "<br>\n",
    "\n",
    "### (2) Fine-tuning 과정 : 학습 과정\n",
    " - 인풋 x를 통해 hidden layer들을 거쳐서 아웃풋 y를 구하고, loss를 구한 뒤 BP로 weight를 업데이트를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Xavier / He initialization\n",
    " - 요즘에 사용되는 방법이다! 신식 버전\n",
    " - 예전에는 랜덤하게 Weight를 초기화 했지만, 이 방법들은 레이어의 특성을 반영하여 Weight를 초기화 하는 방법이다.\n",
    "\n",
    "### (1) Xavier Normal Initialization\n",
    " - Xavier Normal Initialization 수식을 이용해서 초기화!\n",
    " - https://www.youtube.com/watch?v=CJB0g_i7pYk&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=16&t=0s\n",
    "\n",
    "<br>\n",
    "\n",
    "### (2) Xavier Uniform Initialization\n",
    " - Xavier Uniform Initialization 수식을 이용해서 초기화!\n",
    " - https://www.youtube.com/watch?v=CJB0g_i7pYk&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=16&t=0s\n",
    "\n",
    "<br>\n",
    "\n",
    "### (3) He Normal Initialization\n",
    " - He Normal Initialization 수식을 이용해서 초기화!\n",
    " - https://www.youtube.com/watch?v=CJB0g_i7pYk&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=16&t=0s\n",
    "\n",
    "<br>\n",
    "\n",
    "### (4) He Uniform Initialization\n",
    " - He Uniform Initialization 수식을 이용해서 초기화!\n",
    " - https://www.youtube.com/watch?v=CJB0g_i7pYk&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=16&t=0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Xavier의 Package Code 살펴보기 (잘 몰라도 됨)\n",
    " - PyTorch 공식 사이트에 있는 Xavier의 Package의 Code를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " def xavier_uniform_(tensor, gain=1):\n",
    "    fan_in, fan_out = _calculate_fan_in_and_out(tensor) # in, out\n",
    "  \n",
    "    # 수식\n",
    "    std = gain * math.sqrt(2.0 / (fan_in + fan_out))\n",
    "    a = math.sqrt(3.0) * std\n",
    "    with torch.no_grad():\n",
    "        return tensor_uniform_(-a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MNIST - xavier 초기화 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1397, -0.1423, -0.1450,  ..., -0.0837,  0.1299, -0.0135],\n",
       "        [-0.0553,  0.0520, -0.0366,  ...,  0.1125, -0.0583, -0.0505],\n",
       "        [-0.0778, -0.0286,  0.0557,  ..., -0.0576,  0.1133,  0.1207],\n",
       "        ...,\n",
       "        [-0.1251,  0.0872, -0.1491,  ...,  0.0536,  0.0871,  0.1275],\n",
       "        [ 0.0254, -0.1377, -0.1497,  ..., -0.0620, -0.1025,  0.1187],\n",
       "        [ 0.1143,  0.1316, -0.0845,  ...,  0.0999,  0.0270,  0.1307]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 기존에 배운 것\n",
    "linear1 = torch.nn.Linear(784, 256, bias = True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias = True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias = True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# 기존에서 바뀐 부분!\n",
    "torch.nn.init.xavier_uniform_(linear1.weight) #linear1의 weight를 xavier uniform으로 초기화!\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "# ... 생략\n",
    "\n",
    "# 결과 : Accuracy 98%! Weight 초기화만 normal에서 xavier로 바꿨는데 성능 상승!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. MNIST - xavier 초기화 딥하고 넓은 레이어에 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0820, -0.0273,  0.0593,  ...,  0.0448,  0.0353, -0.0304],\n",
       "        [ 0.0374, -0.0358,  0.0603,  ...,  0.1029,  0.0362, -0.0596],\n",
       "        [-0.0165, -0.0536,  0.0156,  ...,  0.0945,  0.0068, -0.0707],\n",
       "        ...,\n",
       "        [ 0.0772, -0.0755, -0.0867,  ...,  0.0717, -0.0840, -0.0009],\n",
       "        [ 0.0502,  0.0402, -0.1059,  ...,  0.0076,  0.0457,  0.0349],\n",
       "        [-0.0566,  0.0526,  0.0467,  ...,  0.0353,  0.0378,  0.0896]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 MLP\n",
    "linear1 = torch.nn.Linear(784, 512, bias = True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias = True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias = True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias = True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias = True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# xavier uniform 초기화!\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "\n",
    "# ... 생략\n",
    "\n",
    "# 결과 : Accuracy 98.1%! 이전보다 조금 더 향상!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
