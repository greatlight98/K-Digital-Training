{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Maximum Likelihood Estimation (MLE)\n",
    " - 최대 우도 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overfitting\n",
    " - 주어진 데이터에 대해 과도하게 학습된 것.\n",
    " - 주어진 데이터를 잘 설명하는 것은 좋으나, 쓸 때 없이 과도하게 설명하는 것은 좋지 않음!\n",
    " - Overfitting 피하는 법 : 훈련 셋 / Development set(Validation Set) / 테스트 셋을 나눠서 이용한다. 훈련 셋이 과적합 되었는지 확인하기 위해 테스트 셋을 이용하고, 테스트 셋 또한 과적합 될 수 있으므로 이를 방지/확인 하기 위해 Dev Set(Validation set)을 이용한다.\n",
    " - Epoch에 따른 Train set의 Loss와 Validation Set의 Loss를 확인 해 보면, 이런 경우가 있다. Train Set의 Loss는 훈련에 따라 계속 낮아지는데, Validation set의 Loss는 일정 값에서 다시 Loss가 증가하게 되는 경우가 있다. 이 때 그 증가하는 지점부터 Overfitting이 되고 있는 것일 알 수 있다. 우리는 Validation Set의 Loss가 가장 작은 지점의 Epoch을 선택하면 된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preventing Overfitting\n",
    " - Data를 많이 모으기 : 데이터가 적을 수록 편향될 가능성이 높다.\n",
    " - feature를 적게 사용하기 : 사람의 얼굴을 설명할 때 특정 데이터를 통해 얼굴에 주근깨가 있고, 이빨이 뾰족하고, 눈썹이 짙고 ... 와 같은 feature들을 다 취하면 그 데이터에 맞게 오버피팅 가능성 높아짐.\n",
    " - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization\n",
    " - Early Stopping : Validation Loss가 더이상 낮아지지 않는 지점을 취한다.\n",
    " - Neural Network의 Size 줄이기\n",
    " - Weight Decay : NN의 Weight Parameter 제한\n",
    " - Dropout : 딥러닝 이용시 사용방법 1 (중요)\n",
    " - Batch Normalization : 딥러닝 이용시 사용방법 2 (중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. DNN 과정 훑어보기\n",
    " - (1) NN 구조 설계 : 입력 데이터, 피쳐 수 등 설계\n",
    " - (2) 훈련 시작 : 오버 피팅이 되기 전까지 깊고 넓게 사이즈를 늘려간다. 오버피팅이 되면 Regularization 등을 이용한다.\n",
    " - (3) 2번 반복!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 이론 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝 셋 (x_train : m * 3, y_train : m)\n",
    "x_train = torch.FloatTensor([[1, 2, 1],\n",
    "                             [1, 3, 2],\n",
    "                             [1, 3, 4],\n",
    "                             [1, 5, 5],\n",
    "                             [1, 7, 5],\n",
    "                             [1, 2, 5],\n",
    "                             [1, 6, 6],\n",
    "                             [1, 7, 7],\n",
    "                            ])\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
    "\n",
    "# 테스트 셋 (x_test : m * 3, y_test : m)\n",
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3) # 3 차원을 받아 3개로 classification 함\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) # linear를 통과시키는 모델임\n",
    "\n",
    "model = SoftmaxClassifierModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        prediction = model(x_train)\n",
    "    \n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "\n",
    "def test(model, optimizer, x_text, y_test):\n",
    "    prediction = model(x_test) # test에 대한 예측값\n",
    "    predicted_calsses = prediction.max(1)[1]\n",
    "    correct_count = (predicted_calsses == y_test).sum().item()\n",
    "    cost = F.cross_entropy(prediction, y_test) # test에 대한 정답값과 예측값을 cross_entropy 실행 \n",
    "  \n",
    "    print('Accuracy: {}% Cost: {:.6f}'.format(correct_count / len(y_test) * 100, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.534081\n",
      "Epoch    1/20 Cost: 1.211772\n",
      "Epoch    2/20 Cost: 1.179600\n",
      "Epoch    3/20 Cost: 1.163372\n",
      "Epoch    4/20 Cost: 1.149543\n",
      "Epoch    5/20 Cost: 1.137342\n",
      "Epoch    6/20 Cost: 1.125898\n",
      "Epoch    7/20 Cost: 1.114987\n",
      "Epoch    8/20 Cost: 1.104458\n",
      "Epoch    9/20 Cost: 1.094251\n",
      "Epoch   10/20 Cost: 1.084329\n",
      "Epoch   11/20 Cost: 1.074676\n",
      "Epoch   12/20 Cost: 1.065276\n",
      "Epoch   13/20 Cost: 1.056122\n",
      "Epoch   14/20 Cost: 1.047205\n",
      "Epoch   15/20 Cost: 1.038520\n",
      "Epoch   16/20 Cost: 1.030059\n",
      "Epoch   17/20 Cost: 1.021816\n",
      "Epoch   18/20 Cost: 1.013787\n",
      "Epoch   19/20 Cost: 1.005964\n",
      "Accuracy: 33.33333333333333% Cost: 0.915790\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "train(model, optimizer, x_train, y_train) # Loss : 0.98\n",
    "test(model, optimizer, x_test, y_test) # Loss : 1.42\n",
    "\n",
    "# 결과 : train set에서는 Loss가 Epoch에 따라 계속 내려가지만, test set에서는 Loss가 그만큼 내려가지 못하고 조금 올라와 있는 것을 알 수 있다. 오버피팅 발생 한 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Learning Rate : lr이 너무 큰 경우\n",
    " - Gradient descent는 lr 값에 따라 업데이트 된다.\n",
    " - lr이 너무 큰 경우 : lr 값이 너무 크면 기울기를 깎아 내려가지 못하고 반대편으로 팅겨 점차 더 높은 기울기로 점차 발산해버릴 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.488147\n",
      "Epoch    1/20 Cost: 993807.937500\n",
      "Epoch    2/20 Cost: 2219313.500000\n",
      "Epoch    3/20 Cost: 179745.453125\n",
      "Epoch    4/20 Cost: 1783376.250000\n",
      "Epoch    5/20 Cost: 1467022.500000\n",
      "Epoch    6/20 Cost: 770876.125000\n",
      "Epoch    7/20 Cost: 2003183.000000\n",
      "Epoch    8/20 Cost: 352960.000000\n",
      "Epoch    9/20 Cost: 1189717.250000\n",
      "Epoch   10/20 Cost: 1151397.500000\n",
      "Epoch   11/20 Cost: 1120876.125000\n",
      "Epoch   12/20 Cost: 1706308.000000\n",
      "Epoch   13/20 Cost: 687335.000000\n",
      "Epoch   14/20 Cost: 917842.187500\n",
      "Epoch   15/20 Cost: 835772.500000\n",
      "Epoch   16/20 Cost: 1470876.125000\n",
      "Epoch   17/20 Cost: 1409433.000000\n",
      "Epoch   18/20 Cost: 1034938.625000\n",
      "Epoch   19/20 Cost: 703779.687500\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e5) # lr이 너무 큰 경우\n",
    "train(model, optimizer, x_train, y_train) # cost = 1198379 (발산해버림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Learning Rate : lr이 너무 작은 경우\n",
    " - lr이 너무 작은 경우 : 기울기를 원하는 방향으로 깎아내려가기는 하지만, 너무 작게 깎아 내려가서 loss 변화가 적음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 2.554362\n",
      "Epoch    1/20 Cost: 2.554362\n",
      "Epoch    2/20 Cost: 2.554362\n",
      "Epoch    3/20 Cost: 2.554362\n",
      "Epoch    4/20 Cost: 2.554362\n",
      "Epoch    5/20 Cost: 2.554362\n",
      "Epoch    6/20 Cost: 2.554362\n",
      "Epoch    7/20 Cost: 2.554362\n",
      "Epoch    8/20 Cost: 2.554362\n",
      "Epoch    9/20 Cost: 2.554362\n",
      "Epoch   10/20 Cost: 2.554362\n",
      "Epoch   11/20 Cost: 2.554362\n",
      "Epoch   12/20 Cost: 2.554362\n",
      "Epoch   13/20 Cost: 2.554362\n",
      "Epoch   14/20 Cost: 2.554362\n",
      "Epoch   15/20 Cost: 2.554362\n",
      "Epoch   16/20 Cost: 2.554362\n",
      "Epoch   17/20 Cost: 2.554362\n",
      "Epoch   18/20 Cost: 2.554362\n",
      "Epoch   19/20 Cost: 2.554362\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-10) # lr이 너무 작은 경우\n",
    "train(model, optimizer, x_train, y_train) # cost = 3.187324 (Epoch에 따른 cost 변화가 크게 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Learning Rate\n",
    " - lr을 잘 설정해야 한다. 데이터와 모델에 따라서 잘 설정해야 하기 때문에 정해진 값은 없고, 여러번 테스트 해서 최적의 값을 찾아야 한다.\n",
    " - lr을 처음에 0.1로 시도해 보고, 추이에 따라 cost가 발산하면 작게, cost가 안줄어들면 크게 조절한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data Preprocessing (데이터 전처리)\n",
    " - 모델이 학습을 잘 하게 데이터를 전처리 하는 것이 중요하다.\n",
    " - ex) 데이터가 [[1000, 0.1], [999, 0.2] ...]와 같다면 데이터의 두 컬럼 모두 중요한데, NN은 1000과 999만 중요하게 생각할 것이다. 따라서 이 컬럼들을 학습 하는데 온 힘을 쏟게 될 것이다. 하지만 이것을 우리가 미리 전처리 해 놓는다면, 두 컬럼 다 똑같은 범위의 값으로 만들 수 있기 때문에 NN이 두 컬럼 다 동등하게 학습할 수 있다.  \n",
    " - 아래는 데이터 전처리 중 standardization(정규분포)의 방법이다.\n",
    " - 우리의 데이터를 mu와 sigma를 통해 정규분포화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n",
      "Epoch    0/20 Cost: 40642.601562\n",
      "Epoch    1/20 Cost: 12740.831055\n",
      "Epoch    2/20 Cost: 3995.108643\n",
      "Epoch    3/20 Cost: 1253.787842\n",
      "Epoch    4/20 Cost: 394.529205\n",
      "Epoch    5/20 Cost: 125.196548\n",
      "Epoch    6/20 Cost: 40.774963\n",
      "Epoch    7/20 Cost: 14.312657\n",
      "Epoch    8/20 Cost: 6.017934\n",
      "Epoch    9/20 Cost: 3.417548\n",
      "Epoch   10/20 Cost: 2.602094\n",
      "Epoch   11/20 Cost: 2.346139\n",
      "Epoch   12/20 Cost: 2.265543\n",
      "Epoch   13/20 Cost: 2.239904\n",
      "Epoch   14/20 Cost: 2.231487\n",
      "Epoch   15/20 Cost: 2.228489\n",
      "Epoch   16/20 Cost: 2.227191\n",
      "Epoch   17/20 Cost: 2.226389\n",
      "Epoch   18/20 Cost: 2.225798\n",
      "Epoch   19/20 Cost: 2.225224\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "mu = x_train.mean(dim=0) # mu\n",
    "sigma = x_train.std(dim=0) # sigma\n",
    "norm_x_train = (x_train - mu) / sigma # mu와 sigma를 통해 normalization 한다.\n",
    "print(norm_x_train) # [-1.2, 0.5, 0.8] ...\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module): # nn.Module 상속받아서 이용\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 독립변수로 3차원을 입력받아 종속변수로 1개의 class를 분류 \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MultivariateLinearRegressionModel() # 모델 선언\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "#def train(model, optimizer, x_train, y_train):\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
