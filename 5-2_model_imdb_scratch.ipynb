{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-2_model_imdb_scratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOye5Dt4Q3CTveRWxRMC3G8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2XWOHqwyv9Kn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1593193785731,"user_tz":-540,"elapsed":4043,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"2624bca6-d525-4c5d-a7b2-4ffb04f4440b"},"source":["! pip list | grep \"torch\""],"execution_count":13,"outputs":[{"output_type":"stream","text":["torch                    1.5.1+cu101    \n","torchsummary             1.5.1          \n","torchtext                0.3.1          \n","torchvision              0.6.1+cu101    \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aJGwgEKR_GPg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593193218656,"user_tz":-540,"elapsed":974,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["import re\n","import sys\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchtext import data\n","from torchtext import datasets"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3O6ttcHKrIq","colab_type":"text"},"source":["## Reading Data"]},{"cell_type":"code","metadata":{"id":"1oJjy5xBJXAx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593193202383,"user_tz":-540,"elapsed":41518,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"54ba92fa-db22-4f17-f175-737ce1f1fe7f"},"source":["\n","# Data Setting\n","TEXT = data.Field(batch_first = True,\n","                  fix_length = 500,\n","                  tokenize=str.split,\n","                  pad_first=True,\n","                  pad_token='[PAD]',\n","                  unk_token='[UNK]')\n","\n","LABEL = data.LabelField(dtype=torch.float)\n","\n","train_data, test_data = datasets.IMDB.splits(text_field = TEXT, \n","                                             label_field = LABEL)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.2MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"34DoliQ6mdob","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593193204453,"user_tz":-540,"elapsed":998,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"4506a83c-9078-497b-ee63-e0819e53e963"},"source":["# Data Length\n","print(f'Train Data Length : {len(train_data.examples)}')\n","print(f'Test Data Length : {len(test_data.examples)}')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Train Data Length : 25000\n","Test Data Length : 25000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3UNJJZE0mH75","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593193204454,"user_tz":-540,"elapsed":507,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"d759aad6-3367-4ad1-d093-8f47b89d814c"},"source":["# Data Fields\n","train_data.fields"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': <torchtext.data.field.LabelField at 0x7f5679fcab38>,\n"," 'text': <torchtext.data.field.Field at 0x7f5679fcabe0>}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"ViUZ13AM_HFQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1593193205912,"user_tz":-540,"elapsed":1089,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"1eb1bb98-2c37-46f4-b641-c48f65623cda"},"source":["# Data Sample\n","print('---- Data Sample ----')\n","print('Input : ')\n","print(' '.join(vars(train_data.examples[1])['text']),'\\n')\n","print('Label : ')\n","print(vars(train_data.examples[1])['label'])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["---- Data Sample ----\n","Input : \n","Depardieu's most notorious film is this (1974)groundbreaker from Bertrand Blier. It features many highly sexual scenes verging on an X-rating, including one of Jeanne Moreau doing a hot 1970s version of her Jules and Jim menage a trois with the two hairy French hippies (Depardieu and Deware). There is no such thing as a sacred territory in this film; everything is fair game.<br /><br />It's very odd that Americans tend to not like this film very much while many French people I've met consider it a classic. Something about it goes against what Americans have been programmed to 'like.'<br /><br />Gerard and the late Patrick Deware are two bitch-slapping, hippy drifters with many sexual insecurities, going around molesting women and committing petty crimes. They're out for kicks and anti-capitalist, Euro-commie, slacker 'freedom.' Blier satirizes the hell out of these two guys while at the same time making bourgeois society itself look ultimately much more ridiculous. Best of all though, is the way the wonderful Stephane Grappelli score conveys the restless soul of the drifters, the deeper subconscious awareness or 'higher ideal' that motivates all the follies they engage in. \n","\n","Label : \n","pos\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pn4ddGIQLL_u","colab_type":"text"},"source":["## Pre-processing Data"]},{"cell_type":"code","metadata":{"id":"gpIQqgb_G41G","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593193212853,"user_tz":-540,"elapsed":976,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["def PreProcessingText(input_sentence):\n","    input_sentence = input_sentence.lower() # 소문자화\n","    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n","    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n","    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n","    if input_sentence:\n","        return input_sentence"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cIqIf34a2SR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593193227668,"user_tz":-540,"elapsed":6108,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["for example in train_data.examples:\n","    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n","    \n","for example in test_data.examples:\n","    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSucMR06LR8Y","colab_type":"text"},"source":["## Making Vocab & Setting Embedding"]},{"cell_type":"code","metadata":{"id":"NBI0uTQUSbdt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593193228570,"user_tz":-540,"elapsed":901,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["model_config = {'emb_type' : '', 'emb_dim' : 300}"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LGSnQIsAHcv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593193947508,"user_tz":-540,"elapsed":2903,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["# no pre-trained \n","TEXT.build_vocab(train_data,\n","                 min_freq = 2, \n","                 max_size = None,\n","                 )\n","\n","LABEL.build_vocab(train_data)\n","\n","model_config['vocab_size'] = len(TEXT.vocab)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"vt7ulofTn6U4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1593193950818,"user_tz":-540,"elapsed":972,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"6a5b9a8c-6f08-4097-d30a-29d0ab8ac582"},"source":["# Vocabulary Info\n","print(f'Vocab Size : {len(TEXT.vocab)}')\n","\n","print('Vocab Examples : ')\n","for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n","    if idx >= 10:\n","        break    \n","    print('\\t', k, v)\n","\n","print('---------------------------------')\n","\n","# Label Info\n","print(f'Label Size : {len(LABEL.vocab)}')\n","\n","print('Lable Examples : ')\n","for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n","    print('\\t', k, v)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Vocab Size : 51956\n","Vocab Examples : \n","\t [UNK] 0\n","\t [PAD] 1\n","\t the 2\n","\t and 3\n","\t a 4\n","\t of 5\n","\t to 6\n","\t is 7\n","\t in 8\n","\t it 9\n","---------------------------------\n","Label Size : 2\n","Lable Examples : \n","\t neg 0\n","\t pos 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zuSh9SlQLfGp","colab_type":"text"},"source":["## Spliting Validation Data & Making Data Iterator"]},{"cell_type":"code","metadata":{"id":"CbH-Rha8___V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593194358662,"user_tz":-540,"elapsed":1050,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["# Spliting Valid set\n","train_data, valid_data = train_data.split(random_state = random.seed(0),\n","                                          split_ratio=0.8)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKOfVNAwAJLU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593194359983,"user_tz":-540,"elapsed":1216,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}}},"source":["model_config['batch_size'] = 30\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data), \n","                                                                           batch_size=model_config['batch_size'],\n","                                                                           device=device)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OFTE-xAwJYb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1592906338611,"user_tz":-540,"elapsed":61452,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"c58f837f-f1f2-44a2-f04e-77fdb9d571ea"},"source":["# Check batch data\n","sample_for_check = next(iter(train_iterator))\n","print(sample_for_check)\n","print(sample_for_check.text)\n","print(sample_for_check.label)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","[torchtext.data.batch.Batch of size 30]\n","\t[.text]:[torch.cuda.LongTensor of size 30x500 (GPU 0)]\n","\t[.label]:[torch.cuda.FloatTensor of size 30 (GPU 0)]\n","tensor([[   1,    1,    1,  ...,   43,    5,  155],\n","        [   1,    1,    1,  ...,   25,    6,  132],\n","        [   1,    1,    1,  ...,   40, 1041,   75],\n","        ...,\n","        [   1,    1,    1,  ...,    5,   65, 1258],\n","        [ 100,  462,   58,  ...,    3,  148,   56],\n","        [   1,    1,    1,  ...,  979,  719,  123]], device='cuda:0')\n","tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n","        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NU1fESnXyR9E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1592906338613,"user_tz":-540,"elapsed":61438,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"6f981b3d-0a7c-401f-9fd8-939147741161"},"source":["# Check reverting data\n","print(' '.join([TEXT.vocab.itos[int(x)] for x in sample_for_check.text[0,:] if x not in [0,1]]))\n","print(LABEL.vocab.itos[int(sample_for_check.label[0])]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["i'm a fan of both shakespeare and mst3k so i waited anxiously to see this episode i'll comment on the movie first then the mst3k episode the recipe for this movie take talented actors rich and beautiful shakespeare material and a 1 25 budget mix well then drain of all life and movement until dull and lifeless serve cold in a big plain stone cauldron movie i give 3 out of 10 because the actors at least deserve a little bit of credit okay now the mst3k episode i'll admit it the first time i saw it i fell asleep halfway through i understand that was the reaction of several other as well however when i watched it a second time i realized that there was a whole host of intelligent references and good lines i missed the first time around the trick with this episode is listen carefully it takes a couple of viewings to catch each line give it a second chance and you'll see what i mean mst3k episode 7 1 2 out of 10\n","neg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cZXWakE7MxU8","colab_type":"text"},"source":["## Making Model"]},{"cell_type":"code","metadata":{"id":"CD2AK0NMAKr1","colab_type":"code","colab":{}},"source":["class SentenceClassification(nn.Module):\n","    def __init__(self, **model_config):\n","        super(SentenceClassification, self).__init__()\n","\n","        if model_config['emb_type'] == 'glove' or 'fasttext':\n","            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],\n","                                    embedding_dim = model_config['emb_dim'],\n","                                    _weight = TEXT.vocab.vectors)\n","        else:\n","            self.emb = nn.Embedding(num_embeddings = model_config['vocab_size'],\n","                                    embedding_dim = model_config['emb_dim'])\n","        \n","        self.bidirectional = model_config['bidirectional']\n","        self.num_direction = 2 if model_config['bidirectional'] else 1\n","        self.model_type = model_config['model_type'] \n","\n","        self.RNN = nn.RNN (input_size = model_config['emb_dim'],\n","                           hidden_size = model_config['hidden_dim'],\n","                           dropout = model_config['dropout'],\n","                           bidirectional = model_config['bidirectional'],\n","                           batch_first = model_config['batch_first'])\n","        \n","        self.LSTM= nn.LSTM(input_size = model_config['emb_dim'],\n","                           hidden_size = model_config['hidden_dim'],\n","                           dropout = model_config['dropout'],\n","                           bidirectional = model_config['bidirectional'],\n","                           batch_first = model_config['batch_first'])\n","        \n","        self.GRU = nn.GRU (input_size = model_config['emb_dim'],\n","                           hidden_size = model_config['hidden_dim'],\n","                           dropout = model_config['dropout'],\n","                           bidirectional = model_config['bidirectional'],\n","                           batch_first = model_config['batch_first'])\n","    \n","        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction,\n","                            model_config['output_dim'])\n","        \n","        self.drop = nn.Dropout(model_config['dropout'])\n","\n","    def forward(self, x):\n","        \n","        emb = self.emb(x) \n","        # emb : (Batch_Size, Max_Seq_Length, Emb_dim)\n","\n","        if self.model_type == 'RNN':\n","            output, hidden = self.RNN(emb) \n","        elif self.model_type == 'LSTM':\n","            output, (hidden, cell) = self.LSTM(emb)\n","        elif self.model_type == 'GRU':\n","            output, hidden = self.GRU(emb)\n","        else:\n","            raise NameError('Select model_type in [RNN, LSTM, GRU]')\n","        \n","        # output : (Batch_Size, Max_Seq_Length, Hidden_dim * num_direction) \n","        # hidden : (num_direction, Batch_Size, Hidden_dim)\n","        # hidden의 경우, batch_first 옵션이 안먹는 문제가 있음\n","        \n","        last_output = output[:,-1,:]\n","\n","        # last_output : (Batch_Size, Hidden_dim * num_direction)\n","        return self.fc(self.drop(last_output))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_3DNQR3M30p","colab_type":"text"},"source":["### Checking feed-forward"]},{"cell_type":"code","metadata":{"id":"r3VJ5BuHkm-4","colab_type":"code","colab":{}},"source":["model_config.update(dict(batch_first = True,\n","                         model_type = 'RNN',\n","                         bidirectional = True,\n","                         hidden_dim = 128,\n","                         output_dim = 1,\n","                         dropout = 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWYjr7KQZxXu","colab_type":"code","colab":{}},"source":["model = SentenceClassification(**model_config).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhGNULElZuI_","colab_type":"code","colab":{}},"source":["predictions = model.forward(sample_for_check.text).squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wabzuaD1AKjF","colab_type":"code","colab":{}},"source":["loss_fn = nn.BCEWithLogitsLoss().to(device)\n","\n","def binary_accuracy(preds, y):\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum()/len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdOXGw9ze-lI","colab_type":"code","colab":{}},"source":["loss = loss_fn(predictions, sample_for_check.label)\n","acc = binary_accuracy(predictions, sample_for_check.label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5OBri4AsPVy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1592906338921,"user_tz":-540,"elapsed":61680,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"397eee3e-3b7d-470e-98b8-9eb21bf62c45"},"source":["print(predictions)\n","print(loss, acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 0.5048,  0.4538,  0.2271,  0.0303,  0.3336,  0.0039,  0.1705, -0.5562,\n","        -0.4903,  0.2203, -0.1409, -0.1375,  0.3909,  0.5392,  0.4122,  0.1316,\n","        -0.4174, -0.1250,  0.4200, -0.3405,  0.3079,  0.7391, -0.2441,  0.1625,\n","        -0.0331, -0.1079,  0.1159, -0.7932,  0.0254,  0.4805], device='cuda:0',\n","       grad_fn=<SqueezeBackward0>)\n","tensor(0.6816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.6000, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rswNBOAzoImC","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"WCgs1fdcAKXF","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train() \n","    batch_size = model_params['batch_size']\n","\n","    for idx, batch in enumerate(iterator):\n","        \n","        # Initializing\n","        optimizer.zero_grad()\n","        \n","        # Forward \n","        predictions = model(batch.text).squeeze()\n","        loss = loss_fn(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        sys.stdout.write(\n","                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n","                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n","                    f\"  Loss: {loss.item():.4}\"\\\n","                    f\"  Acc : {acc.item():.4}\"\\\n","                    )\n","\n","        # Backward \n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Update Epoch Performance\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss/len(iterator) , epoch_acc/len(iterator) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zqJ1gkFARwp","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, loss_fn):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    # evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.text).squeeze(1)\n","            loss = loss_fn(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zIA_7QQzoLK1","colab_type":"text"},"source":["### bi-RNN"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n6GtVuVFUHqz","colab":{}},"source":["model_config['model_type'] = 'RNN'\n","model = SentenceClassification(**model_config).to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.BCEWithLogitsLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uTvlbDLnUHq2","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1592906641386,"user_tz":-540,"elapsed":364107,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"3f3f3a88-890f-4e46-df4d-4abb24cbdf92"},"source":["N_EPOCH = 5\n","\n","best_valid_loss = float('inf')\n","model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n","\n","print('---------------------------------')\n","print(f'Model name : {model_name}')\n","print('---------------------------------')\n","\n","for epoch in range(N_EPOCH):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n","    print('')\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), f'./{model_name}.pt')\n","        print(f'\\t Saved at {epoch}-epoch')\n","\n","    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n","    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------\n","Model name : bi-RNN_\n","---------------------------------\n","[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.5298  Acc : 0.7667\n","\t Saved at 0-epoch\n","\t Epoch : 0 | Train Loss : 0.6392 | Train Acc : 0.6268\n","\t Epoch : 0 | Valid Loss : 0.5983 | Valid Acc : 0.6703\n","[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.6271  Acc : 0.6\n","\t Epoch : 1 | Train Loss : 0.5281 | Train Acc : 0.7383\n","\t Epoch : 1 | Valid Loss : 0.6394 | Valid Acc : 0.6211\n","[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.4983  Acc : 0.8\n","\t Epoch : 2 | Train Loss : 0.4999 | Train Acc : 0.7573\n","\t Epoch : 2 | Valid Loss : 0.6128 | Valid Acc : 0.6722\n","[Train] Epoch :  3 [20010 / 20010 (100.0%)]  Loss: 0.2764  Acc : 0.8667\n","\t Epoch : 3 | Train Loss : 0.445 | Train Acc : 0.7922\n","\t Epoch : 3 | Valid Loss : 0.6013 | Valid Acc : 0.7018\n","[Train] Epoch :  4 [20010 / 20010 (100.0%)]  Loss: 0.4749  Acc : 0.8333\n","\t Epoch : 4 | Train Loss : 0.3589 | Train Acc : 0.844\n","\t Epoch : 4 | Valid Loss : 0.6116 | Valid Acc : 0.7256\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"akDRWiykUHq5","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592906671343,"user_tz":-540,"elapsed":394038,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"da834104-d34f-4c5d-f009-78905305042a"},"source":["# Test set\n","model.load_state_dict(torch.load(f'./{model_name}.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n","print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss : 0.608 | Test Acc : 0.6659\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uxG4rgwTo8D7"},"source":["### bi-LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wpW7ui5mUBTk","colab":{}},"source":["model_config['model_type'] = 'LSTM'\n","model = SentenceClassification(**model_config).to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.BCEWithLogitsLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CDN-nLxJUBTo","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"ok","timestamp":1592907058102,"user_tz":-540,"elapsed":780775,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"d2a2703c-d56b-42e7-f858-9ccdb1481325"},"source":["N_EPOCH = 5\n","\n","best_valid_loss = float('inf')\n","model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n","\n","print('---------------------------------')\n","print(f'Model name : {model_name}')\n","print('---------------------------------')\n","\n","for epoch in range(N_EPOCH):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n","    print('')\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), f'./{model_name}.pt')\n","        print(f'\\t Saved at {epoch}-epoch')\n","\n","    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n","    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------\n","Model name : bi-LSTM_\n","---------------------------------\n","[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.4748  Acc : 0.7333\n","\t Saved at 0-epoch\n","\t Epoch : 0 | Train Loss : 0.6152 | Train Acc : 0.6495\n","\t Epoch : 0 | Valid Loss : 0.5472 | Valid Acc : 0.7189\n","[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.673  Acc : 0.6333\n","\t Epoch : 1 | Train Loss : 0.4824 | Train Acc : 0.7651\n","\t Epoch : 1 | Valid Loss : 0.5653 | Valid Acc : 0.69\n","[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.3318  Acc : 0.8\n","\t Saved at 2-epoch\n","\t Epoch : 2 | Train Loss : 0.3392 | Train Acc : 0.8553\n","\t Epoch : 2 | Valid Loss : 0.4323 | Valid Acc : 0.814\n","[Train] Epoch :  3 [20010 / 20010 (100.0%)]  Loss: 0.3083  Acc : 0.8667\n","\t Saved at 3-epoch\n","\t Epoch : 3 | Train Loss : 0.244 | Train Acc : 0.9031\n","\t Epoch : 3 | Valid Loss : 0.4001 | Valid Acc : 0.8289\n","[Train] Epoch :  4 [20010 / 20010 (100.0%)]  Loss: 0.3233  Acc : 0.8333\n","\t Epoch : 4 | Train Loss : 0.1508 | Train Acc : 0.9444\n","\t Epoch : 4 | Valid Loss : 0.4416 | Valid Acc : 0.8328\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d8qjmPU7UBTq","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592907088133,"user_tz":-540,"elapsed":810789,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"70d0f044-f4d0-4b5c-fcfb-f97e418fc4e0"},"source":["# Test set\n","model.load_state_dict(torch.load(f'./{model_name}.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n","print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss : 0.414 | Test Acc : 0.8285\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FfDGBCWRxnB_"},"source":["### bi-GRU"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"khyItx4zxnCA","colab":{}},"source":["model_config['model_type'] = 'GRU'\n","model = SentenceClassification(**model_config).to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.BCEWithLogitsLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"om_7UjicxnCD","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1592907469165,"user_tz":-540,"elapsed":1191798,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"18750447-423e-4d24-f022-113d735fe6cc"},"source":["N_EPOCH = 5\n","\n","best_valid_loss = float('inf')\n","model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n","\n","print('---------------------------------')\n","print(f'Model name : {model_name}')\n","print('---------------------------------')\n","\n","for epoch in range(N_EPOCH):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n","    print('')\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), f'./{model_name}.pt')\n","        print(f'\\t Saved at {epoch}-epoch')\n","\n","    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n","    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------\n","Model name : bi-GRU_\n","---------------------------------\n","[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.5793  Acc : 0.7333\n","\t Saved at 0-epoch\n","\t Epoch : 0 | Train Loss : 0.6023 | Train Acc : 0.6658\n","\t Epoch : 0 | Valid Loss : 0.5319 | Valid Acc : 0.7335\n","[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.4589  Acc : 0.7333\n","\t Epoch : 1 | Train Loss : 0.4822 | Train Acc : 0.7669\n","\t Epoch : 1 | Valid Loss : 0.5393 | Valid Acc : 0.7175\n","[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.2529  Acc : 0.9\n","\t Saved at 2-epoch\n","\t Epoch : 2 | Train Loss : 0.4126 | Train Acc : 0.8121\n","\t Epoch : 2 | Valid Loss : 0.4136 | Valid Acc : 0.8115\n","[Train] Epoch :  3 [20010 / 20010 (100.0%)]  Loss: 0.1481  Acc : 0.9667\n","\t Epoch : 3 | Train Loss : 0.2558 | Train Acc : 0.8972\n","\t Epoch : 3 | Valid Loss : 0.4763 | Valid Acc : 0.8091\n","[Train] Epoch :  4 [20010 / 20010 (100.0%)]  Loss: 0.1691  Acc : 0.9333\n","\t Epoch : 4 | Train Loss : 0.1581 | Train Acc : 0.9412\n","\t Epoch : 4 | Valid Loss : 0.4271 | Valid Acc : 0.842\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZOL9MoBWxnCG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592907499601,"user_tz":-540,"elapsed":1222229,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"13add090-697c-4acb-fa54-285ca1746fbe"},"source":["# Test set\n","model.load_state_dict(torch.load(f'./{model_name}.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n","print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss : 0.416 | Test Acc : 0.8117\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GEWK8wnkuEzA","colab_type":"text"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"x5VKyllQuReq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592907499602,"user_tz":-540,"elapsed":1222227,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"d888c44d-7dda-4c1b-b284-dcbd41458ffc"},"source":["model_config['model_type'] = 'GRU'\n","model = SentenceClassification(**model_config).to(device)\n","model.load_state_dict(torch.load(f\"./{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}.pt\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"MPifO6k5ugGG","colab_type":"code","colab":{}},"source":["def predict_sentiment(model, sentence):\n","    model.eval()\n","    indexed = TEXT.numericalize(TEXT.pad([TEXT.tokenize(PreProcessingText(sentence))]))\n","    input_data = torch.LongTensor(indexed).to(device)\n","    prediction = torch.sigmoid(model(input_data))\n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gY4lDbr-xB7V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592907499604,"user_tz":-540,"elapsed":1222223,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"e691632a-5c3c-4e20-a90d-5d84762a8c3d"},"source":["test_sentence = 'this movie is FUN'\n","predict_sentiment(model = model, sentence = test_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9888041615486145"]},"metadata":{"tags":[]},"execution_count":35}]}]}