{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vector, Matrix and Tensor\n",
    "### (1) Vector, Matrix and Tensor\n",
    " - 벡터 : 1차원으로 이루어진 행렬\n",
    " - 행렬 : 2차원으로 이루어진 행렬\n",
    " - 텐서 : 3차원으로 이루어진 행렬\n",
    " \n",
    " <br>\n",
    "\n",
    "### (2) PyTorch Tensor Shape Convection\n",
    "#### 1) 2D에서의 텐서\n",
    " - |t| = (batch size(세로) * dimension(가로))\n",
    " \n",
    " ex) |t| = batch size(64) * dim(256)\n",
    "\n",
    "#### 2) 3D에서의 텐서\n",
    " - |t| = (batch size(세로) * width(가로) * height(깊이))\n",
    "\n",
    "#### 3) 3D에서의 텐서 - NLP\n",
    " - |t| = (batch size(세로) * length(가로) * dim(깊이))\n",
    " - dim * length가 batch size만큼 쌓여 있다.\n",
    " \n",
    "<br>\n",
    " \n",
    "<br>\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NumPy Review\n",
    " - NumPy와 PyTorch는 작성법이 아주 유사하고, 직관적이라서 이해가 쉽다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #넘파이\n",
    "import torch #파이토치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 1D Array with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n",
      "Rank of t:  1\n",
      "Shape of t:  (7,)\n",
      "t[0] t[1] t[-1] =  0.0 1.0 6.0\n",
      "t[2:5] t[4:-1] =  [2. 3. 4.] [4. 5.]\n",
      "t[:2] t[3:] =  [0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.]) #0~6을 리스트로 선언하여 array로 만듦\n",
    "print(t)\n",
    "\n",
    "print('Rank of t: ', t.ndim) #t는 1차원 벡터\n",
    "print('Shape of t: ', t.shape) #t에는 1차원이 7개 들어있음\n",
    "\n",
    "print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) #요소 확인\n",
    "print('t[2:5] t[4:-1] = ', t[2:5], t[4:-1]) #Slicing으로 요소 확인\n",
    "print('t[:2] t[3:] = ', t[:2], t[3:]) #Slicing으로 요소 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 2D Array with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n",
      "Rank of t:  2\n",
      "Shape of t:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]]) #2차원 리스트를 선언하여 array로 만듦\n",
    "print(t)\n",
    "\n",
    "print('Rank of t: ', t.ndim) #t는 2차원 행렬\n",
    "print('Shape of t: ', t.shape) #t에는 4*3의 요소가 들어있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PyTorch Tensor Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 1D Array with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.]) #1차원 리스트를 선언하여 floattensor로 만듦\n",
    "print(t)\n",
    "\n",
    "print(t.dim())  # 1차원\n",
    "print(t.shape)  # 1차원 7개의 요소\n",
    "print(t.size()) # 1차원 7개의 요소\n",
    "print(t[0], t[1], t[-1])  # 요소 확인\n",
    "print(t[2:5], t[4:-1])    # Slicing\n",
    "print(t[:2], t[3:])       # Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 2D Array with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ]) #2차원 리스트를 선언하여 floattensor로 만듦\n",
    "print(t)\n",
    "\n",
    "print(t.dim())  # 2차원\n",
    "print(t.size()) # 2차원 4*3 행렬\n",
    "print(t[:, 1]) # 모든 행 중 첫번째 열만 찍어보기\n",
    "print(t[:, 1].size()) # 크기\n",
    "print(t[:, :-1]) # 모든 행 중 마지막 열 전까지만 직어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Broadcasting\n",
    " - 규칙 1 : 매트릭스 연산을 할 때는 두 텐서간의 크기가 같아야 한다.\n",
    " - 규칙 2 : 행렬 곱을 할 때 첫 번째 행렬의 마지막 차원과 두 번째 행렬의 첫번째 차원이 일치해야 한다.\n",
    " - 크기가 다른 텐서 간의 매트릭스 연산을 해야할 때, PyTorch는 Broadcasting 기능을 갖고 있어서 자동으로 크기를 맞춰서 계산하게 해준다.\n",
    " - 주의 : PyTorch가 '자동'으로 브로드캐스팅을 수행하기 때문에, 사용자는 행렬이 어떻게 바뀌었는지 왜 바뀌었는지 파악하지 못할 수 있고 브로드캐스팅을 쓰면 안 되는 때에 자동으로 변형될 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n",
      "tensor([[4., 5.]])\n",
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Same shape\n",
    "m1 = torch.FloatTensor([[3, 3]]) # 1*2 크기\n",
    "m2 = torch.FloatTensor([[2, 2]]) # 1*2 크기\n",
    "print(m1 + m2) #같은 크기이므로 계산 가능\n",
    "\n",
    "# Vector + scalar (원래는 크기가 다르므로 연산 불가)\n",
    "m1 = torch.FloatTensor([[1, 2]]) # 1*2 크기\n",
    "m2 = torch.FloatTensor([3]) # 벡터. PyTorch는 3을 [[3, 3]]으로 바꿔 계산하게 해 준다.\n",
    "print(m1 + m2)\n",
    "\n",
    "# 2 x 1 Vector + 1 x 2 Vector (원래는 크기가 다르므로 연산 불가)\n",
    "m1 = torch.FloatTensor([[1, 2]]) # 1*2 크기를 2*2로 자동으로 바꿔준다.\n",
    "m2 = torch.FloatTensor([[3], [4]]) # 2*1 크기 2*2로 자동으로 바꿔준다.\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Matrix Multiplication\n",
    " - 딥러닝은 행렬곱을 굉장히 많이 수행한다.\n",
    " - 행렬곱에서는 첫번째 행렬의 마지막 차원과 두 번째 행렬의 첫번째 차원이 일치해야 계산 가능하다.\n",
    " 크기가 다른 곳에서는 브로드캐스팅이 수행되어 계산 가능해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Mul vs Matmul\n",
      "-------------\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('-------------')\n",
    "print('Mul vs Matmul')\n",
    "print('-------------')\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 * 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 * 1 => 2 * 2 행렬곱 가능해짐!\n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Basic Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Mean\n",
    " - PyTorch에서는 평균을 구하는 법을 제공한다. NumPy에서의 방법과 굉장히 유용하다. (dim과 exc 부분만 차이가 있다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n",
      "Can only calculate the mean of floating types. Got Long instead.\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 실수 행렬\n",
    "t = torch.FloatTensor([1, 2]) #1차원 실수 행렬 선언\n",
    "print(t.mean()) #mean 계산\n",
    "\n",
    "# Can't use mean() on integers\n",
    "t = torch.LongTensor([1, 2]) #1차원 long 행렬 선언. long 행렬에서는 mean 계산이 안될 때도 있다!\n",
    "try:\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "\n",
    "# 2차원 행렬\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "print(t.mean()) # 모든 요소의 평균값\n",
    "print(t.mean(dim=0)) # 행 방향의 dimension을 없앤다. [[1, 2,], [3, 4]] => mean [4, 6] = [2, 3]\n",
    "print(t.mean(dim=1)) # 열 방향의 dimension을 없앤다. [[1, 2], [3, 4]] => mean [3, 7] = [1.5, 3.5]\n",
    "print(t.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "print(t.sum()) # 모든 요소를 더한 값\n",
    "print(t.sum(dim=0)) # 행 방향의 dimension을 없앤다. sum [[1, 2], [3, 4]] => [4, 6]\n",
    "print(t.sum(dim=1)) # 열 방향의 dimension을 없앤다. sum [[1, 2], [3, 4]] => [3, 7]\n",
    "print(t.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Max and Argmax\n",
    " - Max : 가장 큰 값을 찾는다.\n",
    " - Argmax : 가장 큰 값의 인덱스 값을 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(4.)\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "print(t.max()) # 가장 큰 값인 4 리턴\n",
    "\n",
    "print(t.max(dim=0)) # Returns two values: max and argmax\n",
    "print('Max: ', t.max(dim=0)[0]) # 행 방향의 dimension을 없애며 그 중 큰 값을 택한다. [[1, 2], [3, 4]] => [3, 4]\n",
    "print('Argmax: ', t.max(dim=0)[1]) # 큰 값의 인덱스\n",
    "\n",
    "print(t.max(dim=1)) # 열 방향의 dimension을 없애며 그 중 큰 값을 택한다. [[1, 2], [3, 4]] => [2, 4]\n",
    "print(t.max(dim=-1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
