{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Time Series Data (시계열 데이터)\n",
    " - 시계열 데이터 : 일정한 시간 간격으로 배치된 데이터\n",
    " - ex) 주가 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 시계열 데이터 예시 1\n",
    " - 구글 주가 데이터의 7일간 데이터를 통해 8일자 종가를 예측해보자.\n",
    " ### (1) Day 1\n",
    "  - Day 1 : [Open : 828, High : 833, Low : 828, Volume : 1247700, Close : 831]\n",
    "  - Day 1의 5차원 데이터를 RNN의 셀에 넣어 처리하여 다음 셀로 넘겨준다.\n",
    "\n",
    "<br>\n",
    "\n",
    " ### (2) Day 2\n",
    "  - Day 2의 5차원 데이터를 RNN의 셀에 넣고, Day 1의 데이터와 처리하여 다음 셀로 넘겨준다.\n",
    "\n",
    "<br>\n",
    "\n",
    " ### (3) Day 7까지 반복!\n",
    "\n",
    "<br>\n",
    "\n",
    " ### (4) Output\n",
    "  - 8일차 종가를 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 시계열 데이터 예시 2\n",
    " ### (1) 문제점\n",
    "  - 종가는 '하나의 값', 즉 1차원의 값이다.\n",
    "  - Output_size와 Hidden_size는 같으므로, hidden_size도 1차원으로 설정해야 된다.\n",
    "  - 5차원의 데이터를 받아 셀에서 처리하여 1차원의 형태로 다음 셀로 넘기는건 모델에게 굉장히 부담스러운 일이다.\n",
    "\n",
    "<br>\n",
    "\n",
    " ### (2) 해결 방법\n",
    "  - Output_size와 Hidden_size를 모두 10차원(임의로 정의함)으로 처리하게 한다.\n",
    "  - Output을 할 때 FC Layer를 넣어 종가를 예측하도록 처리한다.\n",
    "  - 이 방법이 일반적인 방법이다!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 시계열 데이터와 RNN(LSTM) 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fab69bae570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 라이브러리 로드\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 하이퍼파라미터 설정\n",
    "seq_length = 7 # 7일\n",
    "data_dim = 5 # 데이터는 5차원\n",
    "hidden_dim = 10 # 임의 정의\n",
    "output_dim = 1 # 종가는 1차원\n",
    "learning_rate = 0.01\n",
    "iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터 로드\n",
    "xy = np.loadtxt(\"data-02-stock_daily.csv\", delimiter=\",\")\n",
    "xy = xy[::-1] # 데이터를 역순으로 ordering 해줌\n",
    "\n",
    "train_size = int(len(xy) * 0.7) # 데이터의 70%를 train set으로 이용\n",
    "train_set = xy[0:train_size] # train set\n",
    "test_set = xy[train_size - seq_length:] # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_dataset() missing 6 required positional arguments: 'sample', 'subsample', 'scale', 'batch_size', 'weight_fn', and 'num_workers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1968ffe9c10d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# X : 7일간의 데이터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Y : 8일차 종가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# build_dataset의 함수를 자세히 알고 싶다면 구글링 고고! => 하라고 써 있으나, 구글링 안됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_dataset() missing 6 required positional arguments: 'sample', 'subsample', 'scale', 'batch_size', 'weight_fn', and 'num_workers'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 4. 데이터 전처리\n",
    "'''\n",
    " - 데이터를 보면, 주가는 800대 였고 거래량은 100만대 였다.\n",
    " - 모델이 느끼기에 800과 100만대를 다루는 건 부담스러운 작업이다.\n",
    " - 모델이 느끼기에 800과 100만대 중 100만대를 더 중요하게 생각할 수 있다.\n",
    " - scaler : 값을 0 ~ 1 사이의 상대값으로 전환한다. 최고가와 최저가 사이를 선형으로 나누어 0 ~ 1 의 값으로 만든다.\n",
    "'''\n",
    "# scaler\n",
    "train_set = MinMaxScaler(train_set)\n",
    "test_set = MinMaxScaler(test_set)\n",
    "\n",
    "# X : 7일간의 데이터\n",
    "# Y : 8일차 종가\n",
    "trainX, trainY = build_dataset(train_set, seq_length) # build_dataset의 함수를 자세히 알고 싶다면 구글링 고고! => 하라고 써 있으나, 구글링 안됨\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "# 텐서화\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "textY_tensor = torch.FloatTensor(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RNN 모델(LSTM) 만들어 두기\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "    super(Net, self).__init__()\n",
    "    self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers = layers, batch_first=True) # RNN 모델로 LSTM 이용\n",
    "    self.fc = torch.nn.Linear(hidden_dim, output_dim, bias = True) # FC Layer\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x, _status = self.rnn(x) # RNN 모델\n",
    "    x = self.fc(x[:, -1]) # FC Layer\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. RNN 모델(LSTM) 이용하기\n",
    "net = Net(data_dim, hidden_dim, output_dim, 1) # input_dim, hidden_dim, output_dim, 레이어 수를 설정하여 모델 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Loss와 Optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. RNN 모델(LSTM) 학습하기!\n",
    "for i in range(iteration):\n",
    "  outputs = net(trainX_tensor) # 예측값\n",
    "  loss = criterion(outputs, trainY_tensor) # 예측값과 실제값을 통한 loss 구하기\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. test 시각화\n",
    "plt.plot(testY) # test 실제값 시각화\n",
    "plt.plot(net(testX_tensor).data.numpy()) # test 예측값 시각화\n",
    "plt.legend(['original', 'prediction']) # 축 설정\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
