{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-4_model_imdb_fasttext.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM6aaEyBN0Vx0y5s7xKIAHb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aJGwgEKR_GPg","colab_type":"code","colab":{}},"source":["import re\n","import sys\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchtext import data\n","from torchtext import datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3O6ttcHKrIq","colab_type":"text"},"source":["## Reading Data"]},{"cell_type":"code","metadata":{"id":"1oJjy5xBJXAx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592897540907,"user_tz":-540,"elapsed":45748,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"4bc884b4-a4cb-4702-9882-9c4ced75ad82"},"source":["# Data Setting\n","TEXT = data.Field(batch_first = True,\n","                  fix_length = 500,\n","                  tokenize=str.split,\n","                  pad_first=True,\n","                  pad_token='[PAD]',\n","                  unk_token='[UNK]')\n","\n","LABEL = data.LabelField(dtype=torch.float)\n","\n","train_data, test_data = datasets.IMDB.splits(text_field = TEXT, \n","                                             label_field = LABEL)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.8MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"34DoliQ6mdob","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592897542573,"user_tz":-540,"elapsed":1657,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"77297708-16c8-4127-b0b7-e0801cc48310"},"source":["# Data Length\n","print(f'Train Data Length : {len(train_data.examples)}')\n","print(f'Test Data Length : {len(test_data.examples)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Data Length : 25000\n","Test Data Length : 25000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3UNJJZE0mH75","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592897542574,"user_tz":-540,"elapsed":1646,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"fcf3af23-ed69-4d97-f4fa-27b169676aac"},"source":["# Data Fields\n","train_data.fields"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': <torchtext.data.field.LabelField at 0x7f98c6c69240>,\n"," 'text': <torchtext.data.field.Field at 0x7f98c6c69fd0>}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ViUZ13AM_HFQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1592897542574,"user_tz":-540,"elapsed":1634,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"294ea99f-4fca-482f-d299-0a71da217ee6"},"source":["# Data Sample\n","print('---- Data Sample ----')\n","print('Input : ')\n","print(' '.join(vars(train_data.examples[1])['text']),'\\n')\n","print('Label : ')\n","print(vars(train_data.examples[1])['label'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---- Data Sample ----\n","Input : \n","This is definitely an appropriate update for the original, except that \"party on the left is now party on the right.\" Like the original, this movie rails against a federal government which oversteps its bounds with regards to personal liberty. It is a warning of how tenuous our political liberties are in an era of an over-zealous, and over-powerful federal government. Kowalski serves as a metaphor for Waco and Ruby Ridge, where the US government, with the cooperation of the mainstream media, threw around words like \"white supremacist\" and \"right wing extremists as well as trumped-up drug charges to abridge the most fundamental of its' citizens rights, with the willing acquiescence of the general populace. That message is so non-PC, I am stunned that this film could be made - at least not without bringing the Federal government via the IRS down on the makers like they did to Juanita Broderick, Katherine Prudhomme, the Western Journalism Center, and countless others who dared to speak out. \"Live Free or Die\" is the motto on Jason Priestly's hat as he brilliantly portrays \"the voice,\" and that sums up the dangerous (to some) message of this film.<br /><br /> \n","\n","Label : \n","pos\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pn4ddGIQLL_u","colab_type":"text"},"source":["## Pre-processing Data"]},{"cell_type":"code","metadata":{"id":"gpIQqgb_G41G","colab_type":"code","colab":{}},"source":["def PreProcessingText(input_sentence):\n","    input_sentence = input_sentence.lower() # 소문자화\n","    input_sentence = re.sub('<[^>]*>', repl= ' ', string = input_sentence) # \"<br />\" 처리\n","    input_sentence = re.sub('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]', repl= ' ', string = input_sentence) # 특수문자 처리 (\"'\" 제외)\n","    input_sentence = re.sub('\\s+', repl= ' ', string = input_sentence) # 연속된 띄어쓰기 처리\n","    if input_sentence:\n","        return input_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cIqIf34a2SR","colab_type":"code","colab":{}},"source":["for example in train_data.examples:\n","    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()\n","    \n","for example in test_data.examples:\n","    vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSucMR06LR8Y","colab_type":"text"},"source":["## Making Vocab & Setting Embedding"]},{"cell_type":"code","metadata":{"id":"NBI0uTQUSbdt","colab_type":"code","colab":{}},"source":["model_config = {'emb_type' : 'fasttext', 'emb_dim' : 300}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-y5MMRC7xRw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1592898104969,"user_tz":-540,"elapsed":529277,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"1fdd0c3d-a12e-4736-b447-976cabf1bff2"},"source":["# fasttext pre-trained \n","\n","# TEXT.build_vocab(train_data,\n","#                  min_freq = 2, \n","#                  max_size = None,\n","#                  vectors = 'fasttext.en.300d')\n","\n","# 위와 같은 옵션으로 fasttext Pre-Trained vector를 가져올 수 있으나 현재 버그가 있어서 직접 다운받아 사용하는 방법으로 변경하였습니다.\n","# ISSUE : https://github.com/pytorch/text/issues/634\n","\n","# 아래 코드는 jupyter notebook에서만 실행됩니다. python script로 실행할 경우에는 따로 다운받아서 준비해주시기 바랍니다.\n","! wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec' "],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-23 07:32:57--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6597238061 (6.1G) [binary/octet-stream]\n","Saving to: ‘wiki.en.vec’\n","\n","wiki.en.vec         100%[===================>]   6.14G  7.33MB/s    in 8m 45s  \n","\n","2020-06-23 07:41:43 (12.0 MB/s) - ‘wiki.en.vec’ saved [6597238061/6597238061]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8LGSnQIsAHcv","colab_type":"code","colab":{}},"source":["# fasttext pre-trained \n","from torchtext.vocab import Vectors\n","fasttext_vectors = Vectors('./wiki.en.vec')\n","\n","TEXT.build_vocab(train_data,\n","                 min_freq = 2, \n","                 max_size = None,\n","                 vectors = fasttext_vectors)\n","\n","LABEL.build_vocab(train_data)\n","\n","model_config['vocab_size'] = len(TEXT.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vt7ulofTn6U4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1592898684863,"user_tz":-540,"elapsed":1102821,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"b1d4da6c-1ca7-41aa-efd0-026aac48e322"},"source":["# Vocabulary Info\n","print(f'Vocab Size : {len(TEXT.vocab)}')\n","\n","print('Vocab Examples : ')\n","for idx, (k, v) in enumerate(TEXT.vocab.stoi.items()):\n","    if idx >= 10:\n","        break    \n","    print('\\t', k, v)\n","\n","print('---------------------------------')\n","\n","# Label Info\n","print(f'Label Size : {len(LABEL.vocab)}')\n","\n","print('Lable Examples : ')\n","for idx, (k, v) in enumerate(LABEL.vocab.stoi.items()):\n","    print('\\t', k, v)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocab Size : 51956\n","Vocab Examples : \n","\t [UNK] 0\n","\t [PAD] 1\n","\t the 2\n","\t and 3\n","\t a 4\n","\t of 5\n","\t to 6\n","\t is 7\n","\t in 8\n","\t it 9\n","---------------------------------\n","Label Size : 2\n","Lable Examples : \n","\t neg 0\n","\t pos 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s7dLNrxJMmEF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592898684864,"user_tz":-540,"elapsed":1100464,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"dfa8bc65-f246-4b7f-f5f7-ca7545542c00"},"source":["# Check embedding vectors\n","TEXT.vocab.vectors.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([51956, 300])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"zuSh9SlQLfGp","colab_type":"text"},"source":["## Spliting Validation Data & Making Data Iterator"]},{"cell_type":"code","metadata":{"id":"CbH-Rha8___V","colab_type":"code","colab":{}},"source":["# Spliting Valid set\n","train_data, valid_data = train_data.split(random_state = random.seed(0),\n","                                          split_ratio=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKOfVNAwAJLU","colab_type":"code","colab":{}},"source":["model_config['batch_size'] = 30\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size=model_config['batch_size'],\n","    device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OFTE-xAwJYb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1592898694335,"user_tz":-540,"elapsed":1105565,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"2c2a405b-d81b-465d-f8d0-254e5d62fb50"},"source":["# Check batch data\n","sample_for_check = next(iter(train_iterator))\n","print(sample_for_check)\n","print(sample_for_check.text)\n","print(sample_for_check.label)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","[torchtext.data.batch.Batch of size 30]\n","\t[.text]:[torch.cuda.LongTensor of size 30x500 (GPU 0)]\n","\t[.label]:[torch.cuda.FloatTensor of size 30 (GPU 0)]\n","tensor([[   1,    1,    1,  ...,   43,    5,  155],\n","        [   1,    1,    1,  ...,   25,    6,  132],\n","        [   1,    1,    1,  ...,   40, 1041,   75],\n","        ...,\n","        [   1,    1,    1,  ...,    5,   65, 1258],\n","        [ 100,  462,   58,  ...,    3,  148,   56],\n","        [   1,    1,    1,  ...,  979,  719,  123]], device='cuda:0')\n","tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n","        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NU1fESnXyR9E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1592898694336,"user_tz":-540,"elapsed":1102942,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"3d3324a5-2a49-4da0-bdbc-03ca5c02c155"},"source":["# Check reverting data\n","print(' '.join([TEXT.vocab.itos[int(x)] for x in sample_for_check.text[0,:] if x not in [0,1]]))\n","print(LABEL.vocab.itos[int(sample_for_check.label[0])]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["i'm a fan of both shakespeare and mst3k so i waited anxiously to see this episode i'll comment on the movie first then the mst3k episode the recipe for this movie take talented actors rich and beautiful shakespeare material and a 1 25 budget mix well then drain of all life and movement until dull and lifeless serve cold in a big plain stone cauldron movie i give 3 out of 10 because the actors at least deserve a little bit of credit okay now the mst3k episode i'll admit it the first time i saw it i fell asleep halfway through i understand that was the reaction of several other as well however when i watched it a second time i realized that there was a whole host of intelligent references and good lines i missed the first time around the trick with this episode is listen carefully it takes a couple of viewings to catch each line give it a second chance and you'll see what i mean mst3k episode 7 1 2 out of 10\n","neg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cZXWakE7MxU8","colab_type":"text"},"source":["## Making Model"]},{"cell_type":"code","metadata":{"id":"CD2AK0NMAKr1","colab_type":"code","colab":{}},"source":["class SentenceClassification(nn.Module):\n","    def __init__(self, **model_config):\n","        super(SentenceClassification, self).__init__()\n","\n","        if model_config['emb_type'] == 'glove' or 'fasttext':\n","            self.emb = nn.Embedding(model_config['vocab_size'],\n","                                    model_config['emb_dim'],\n","                                    _weight = TEXT.vocab.vectors)\n","        else:\n","            self.emb = nn.Embedding(model_config['vocab_size'],\n","                                    model_config['emb_dim'])\n","        \n","        self.bidirectional = model_config['bidirectional']\n","        self.num_direction = 2 if model_config['bidirectional'] else 1\n","        self.model_type = model_config['model_type'] \n","\n","        self.RNN = nn.RNN (input_size = model_config['emb_dim'],\n","                           hidden_size = model_config['hidden_dim'],\n","                           dropout = model_config['dropout'],\n","                           bidirectional = model_config['bidirectional'],\n","                           batch_first = model_config['batch_first'])\n","        \n","        self.LSTM= nn.LSTM(input_size = model_config['emb_dim'],\n","                           hidden_size = model_config['hidden_dim'],\n","                           dropout = model_config['dropout'],\n","                           bidirectional = model_config['bidirectional'],\n","                           batch_first = model_config['batch_first'])\n","        \n","        self.GRU = nn.GRU (input_size = model_config['emb_dim'],\n","                           hidden_size = model_config['hidden_dim'],\n","                           dropout = model_config['dropout'],\n","                           bidirectional = model_config['bidirectional'],\n","                           batch_first = model_config['batch_first'])\n","    \n","        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction,\n","                            model_config['output_dim'])\n","        \n","        self.drop = nn.Dropout(model_config['dropout'])\n","\n","    def forward(self, x):\n","        \n","        emb = self.emb(x) \n","        # emb : (Batch_Size, Max_Seq_Length, Emb_dim)\n","\n","        if self.model_type == 'RNN':\n","            output, hidden = self.RNN(emb) \n","        elif self.model_type == 'LSTM':\n","            output, (hidden, cell) = self.LSTM(emb)\n","        elif self.model_type == 'GRU':\n","            output, hidden = self.GRU(emb)\n","        else:\n","            raise NameError('Select model_type in [RNN, LSTM, GRU]')\n","        \n","        # output : (Batch_Size, Max_Seq_Length, Hidden_dim * num_direction) \n","        # hidden : (Batch_Size, num_direction, Hidden_dim)\n","        \n","        last_output = output[:,-1,:]\n","\n","        # last_output : (Batch_Size, Hidden_dim * num_direction)\n","        return self.fc(self.drop(last_output))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_3DNQR3M30p","colab_type":"text"},"source":["### Checking feed-forward"]},{"cell_type":"code","metadata":{"id":"r3VJ5BuHkm-4","colab_type":"code","colab":{}},"source":["model_config.update(dict(batch_first = True,\n","                         model_type = 'RNN',\n","                         bidirectional = True,\n","                         hidden_dim = 128,\n","                         output_dim = 1,\n","                         dropout = 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWYjr7KQZxXu","colab_type":"code","colab":{}},"source":["model = SentenceClassification(**model_config).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhGNULElZuI_","colab_type":"code","colab":{}},"source":["predictions = model.forward(sample_for_check.text).squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wabzuaD1AKjF","colab_type":"code","colab":{}},"source":["loss_fn = nn.BCEWithLogitsLoss().to(device)\n","\n","def binary_accuracy(preds, y):\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum()/len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdOXGw9ze-lI","colab_type":"code","colab":{}},"source":["loss = loss_fn(predictions, sample_for_check.label)\n","acc = binary_accuracy(predictions, sample_for_check.label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5OBri4AsPVy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592898694341,"user_tz":-540,"elapsed":1089630,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"97b784da-c03e-4a43-d228-ad932e419eab"},"source":["print(predictions)\n","print(loss, acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.1428, 0.0793, 0.2120, 0.1943, 0.1225, 0.3271, 0.0696, 0.0818, 0.0368,\n","        0.2749, 0.2509, 0.1662, 0.0908, 0.1364, 0.1094, 0.0711, 0.0253, 0.0784,\n","        0.0629, 0.1540, 0.0161, 0.0567, 0.1145, 0.0319, 0.1674, 0.1520, 0.1143,\n","        0.0493, 0.0863, 0.1644], device='cuda:0', grad_fn=<SqueezeBackward0>)\n","tensor(0.7038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>) tensor(0.5000, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rswNBOAzoImC","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"WCgs1fdcAKXF","colab_type":"code","colab":{}},"source":["\n","def train(model, iterator, optimizer, loss_fn, idx_epoch, **model_params):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train() \n","    batch_size = model_params['batch_size']\n","\n","    for idx, batch in enumerate(iterator):\n","        \n","        # Initializing\n","        optimizer.zero_grad()\n","        \n","        # Forward \n","        predictions = model(batch.text).squeeze()\n","        loss = loss_fn(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        sys.stdout.write(\n","                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n","                    f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size} ({100. * (idx + 1) / len(iterator) :.4}%)]\"\\\n","                    f\"  Loss: {loss.item():.4}\"\\\n","                    f\"  Acc : {acc.item():.4}\"\\\n","                    )\n","\n","        # Backward \n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Update Epoch Performance\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss/len(iterator) , epoch_acc/len(iterator) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zqJ1gkFARwp","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, loss_fn):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    # evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.text).squeeze(1)\n","            loss = loss_fn(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zIA_7QQzoLK1","colab_type":"text"},"source":["### bi-RNN"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n6GtVuVFUHqz","colab":{}},"source":["model_config['model_type'] = 'RNN'\n","model = SentenceClassification(**model_config).to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.BCEWithLogitsLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uTvlbDLnUHq2","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1592901996110,"user_tz":-540,"elapsed":310416,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"6acc7122-b5fa-4594-f259-3629f788899e"},"source":["N_EPOCH = 5\n","\n","best_valid_loss = float('inf')\n","model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n","\n","print('---------------------------------')\n","print(f'Model name : {model_name}')\n","print('---------------------------------')\n","\n","for epoch in range(N_EPOCH):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n","    print('')\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), f'./{model_name}.pt')\n","        print(f'\\t Saved at {epoch}-epoch')\n","\n","    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n","    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------\n","Model name : bi-RNN_fasttext\n","---------------------------------\n","[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.5937  Acc : 0.7333\n","\t Saved at 0-epoch\n","\t Epoch : 0 | Train Loss : 0.5855 | Train Acc : 0.6843\n","\t Epoch : 0 | Valid Loss : 0.6241 | Valid Acc : 0.7313\n","[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.3864  Acc : 0.7667\n","\t Saved at 1-epoch\n","\t Epoch : 1 | Train Loss : 0.507 | Train Acc : 0.754\n","\t Epoch : 1 | Valid Loss : 0.5609 | Valid Acc : 0.719\n","[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.5854  Acc : 0.7\n","\t Epoch : 2 | Train Loss : 0.38 | Train Acc : 0.8347\n","\t Epoch : 2 | Valid Loss : 0.6707 | Valid Acc : 0.5851\n","[Train] Epoch :  3 [20010 / 20010 (100.0%)]  Loss: 0.3719  Acc : 0.8333\n","\t Epoch : 3 | Train Loss : 0.4988 | Train Acc : 0.7408\n","\t Epoch : 3 | Valid Loss : 0.6833 | Valid Acc : 0.639\n","[Train] Epoch :  4 [20010 / 20010 (100.0%)]  Loss: 0.7006  Acc : 0.6333\n","\t Epoch : 4 | Train Loss : 0.3737 | Train Acc : 0.8292\n","\t Epoch : 4 | Valid Loss : 0.7377 | Valid Acc : 0.6136\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"akDRWiykUHq5","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592902025714,"user_tz":-540,"elapsed":340010,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"f5dab1e6-82aa-4524-fc81-f17b7f26171a"},"source":["# Test set\n","model.load_state_dict(torch.load(f'./{model_name}.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n","print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss : 0.5666 | Test Acc : 0.7163\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uxG4rgwTo8D7"},"source":["### bi-LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wpW7ui5mUBTk","colab":{}},"source":["model_config['model_type'] = 'LSTM'\n","model = SentenceClassification(**model_config).to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.BCEWithLogitsLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CDN-nLxJUBTo","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1592901654227,"user_tz":-540,"elapsed":400260,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"807c3fd6-f1f5-407d-c2d4-6a49c637c070"},"source":["N_EPOCH = 5\n","\n","best_valid_loss = float('inf')\n","model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n","\n","print('---------------------------------')\n","print(f'Model name : {model_name}')\n","print('---------------------------------')\n","\n","for epoch in range(N_EPOCH):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n","    print('')\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), f'./{model_name}.pt')\n","        print(f'\\t Saved at {epoch}-epoch')\n","\n","    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n","    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------\n","Model name : bi-LSTM_fasttext\n","---------------------------------\n","[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.9103  Acc : 0.3333\n","\t Saved at 0-epoch\n","\t Epoch : 0 | Train Loss : 0.5922 | Train Acc : 0.6895\n","\t Epoch : 0 | Valid Loss : 0.6722 | Valid Acc : 0.5856\n","[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.436  Acc : 0.8\n","\t Saved at 1-epoch\n","\t Epoch : 1 | Train Loss : 0.3893 | Train Acc : 0.8246\n","\t Epoch : 1 | Valid Loss : 0.3152 | Valid Acc : 0.8736\n","[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.08638  Acc : 0.9333\n","\t Epoch : 2 | Train Loss : 0.1778 | Train Acc : 0.9333\n","\t Epoch : 2 | Valid Loss : 0.3201 | Valid Acc : 0.8787\n","[Train] Epoch :  3 [20010 / 20010 (100.0%)]  Loss: 0.00928  Acc : 1.0\n","\t Epoch : 3 | Train Loss : 0.07141 | Train Acc : 0.9772\n","\t Epoch : 3 | Valid Loss : 0.3808 | Valid Acc : 0.8746\n","[Train] Epoch :  4 [20010 / 20010 (100.0%)]  Loss: 0.07977  Acc : 0.9667\n","\t Epoch : 4 | Train Loss : 0.0275 | Train Acc : 0.9925\n","\t Epoch : 4 | Valid Loss : 0.4877 | Valid Acc : 0.8554\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d8qjmPU7UBTq","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592901685673,"user_tz":-540,"elapsed":429432,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"ee9cb713-d45a-4661-a333-477486cb22c4"},"source":["# Test set\n","model.load_state_dict(torch.load(f'./{model_name}.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n","print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss : 0.321 | Test Acc : 0.8687\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FfDGBCWRxnB_"},"source":["### bi-GRU"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"khyItx4zxnCA","colab":{}},"source":["model_config['model_type'] = 'GRU'\n","model = SentenceClassification(**model_config).to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.BCEWithLogitsLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"om_7UjicxnCD","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1592901170802,"user_tz":-540,"elapsed":384682,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"d1ea9e2d-407d-4fc6-b855-8491e6cb218c"},"source":["N_EPOCH = 5\n","\n","best_valid_loss = float('inf')\n","model_name = f\"{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\"\n","\n","print('---------------------------------')\n","print(f'Model name : {model_name}')\n","print('---------------------------------')\n","\n","for epoch in range(N_EPOCH):\n","    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn, epoch, **model_config)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n","    print('')\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), f'./{model_name}.pt')\n","        print(f'\\t Saved at {epoch}-epoch')\n","\n","    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n","    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["---------------------------------\n","Model name : bi-GRU_fasttext\n","---------------------------------\n","[Train] Epoch :  0 [20010 / 20010 (100.0%)]  Loss: 0.4283  Acc : 0.8667\n","\t Saved at 0-epoch\n","\t Epoch : 0 | Train Loss : 0.4061 | Train Acc : 0.8075\n","\t Epoch : 0 | Valid Loss : 0.2966 | Valid Acc : 0.8759\n","[Train] Epoch :  1 [20010 / 20010 (100.0%)]  Loss: 0.07912  Acc : 1.0\n","\t Saved at 1-epoch\n","\t Epoch : 1 | Train Loss : 0.1576 | Train Acc : 0.9424\n","\t Epoch : 1 | Valid Loss : 0.2522 | Valid Acc : 0.8993\n","[Train] Epoch :  2 [20010 / 20010 (100.0%)]  Loss: 0.01315  Acc : 1.0\n","\t Epoch : 2 | Train Loss : 0.04567 | Train Acc : 0.9851\n","\t Epoch : 2 | Valid Loss : 0.3739 | Valid Acc : 0.8913\n","[Train] Epoch :  3 [20010 / 20010 (100.0%)]  Loss: 0.001646  Acc : 1.0\n","\t Epoch : 3 | Train Loss : 0.01101 | Train Acc : 0.9968\n","\t Epoch : 3 | Valid Loss : 0.4561 | Valid Acc : 0.8895\n","[Train] Epoch :  4 [20010 / 20010 (100.0%)]  Loss: 0.006299  Acc : 1.0\n","\t Epoch : 4 | Train Loss : 0.006686 | Train Acc : 0.9979\n","\t Epoch : 4 | Valid Loss : 0.5505 | Valid Acc : 0.8859\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZOL9MoBWxnCG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592901217312,"user_tz":-540,"elapsed":32022,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"144a2d7b-7ca2-4194-832f-a00b0f791975"},"source":["# Test set\n","model.load_state_dict(torch.load(f'./{model_name}.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n","print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss : 0.278 | Test Acc : 0.8868\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GEWK8wnkuEzA","colab_type":"text"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"x5VKyllQuReq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592902759994,"user_tz":-540,"elapsed":1991,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"93c51e1d-d358-4467-dea2-e7ddf1e8e67f"},"source":["model_config['model_type'] = 'GRU'\n","model = SentenceClassification(**model_config).to(device)\n","model.load_state_dict(torch.load(f\"./{'bi-' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}.pt\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"l35fvQw87-vg","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPifO6k5ugGG","colab_type":"code","colab":{}},"source":["def predict_sentiment(model, sentence):\n","    model.eval()\n","    indexed = TEXT.numericalize(TEXT.pad([TEXT.tokenize(PreProcessingText(sentence))]))\n","    input_data = torch.LongTensor(indexed).to(device)\n","    prediction = torch.sigmoid(model(input_data))\n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gY4lDbr-xB7V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592902765787,"user_tz":-540,"elapsed":1771,"user":{"displayName":"seongsu bang","photoUrl":"","userId":"06124410414614761416"}},"outputId":"985a1922-379f-4321-87fa-9ff34986a3c7"},"source":["test_sentence = 'this movie is FUN'\n","predict_sentiment(model = model, sentence = test_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9115353226661682"]},"metadata":{"tags":[]},"execution_count":62}]}]}