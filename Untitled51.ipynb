{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7002a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    \"\"\" Train dataset을 가져와서 torch 모델이 학습할 수 있는 tensor 형태로 반환합니다.\"\"\"\n",
    "    def __init__(self, path):\n",
    "        super(PandasDataset, self).__init__()\n",
    "        train = pd.read_csv(path).iloc[:,1:]\n",
    "        self.train_X, self.train_Y = train.iloc[:,4:], train.iloc[:,0:4]\n",
    "        self.tmp_x , self.tmp_y = self.train_X.values, self.train_Y.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'X':torch.from_numpy(self.tmp_x)[idx],\n",
    "            'Y':torch.from_numpy(self.tmp_y)[idx]\n",
    "        }\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, path_test):\n",
    "        super(TestDataset, self).__init__()\n",
    "        test = pd.read_csv(path_test)\n",
    "        self.test_X = test.iloc[:,1:]\n",
    "        self.tmp_x = self.test_X.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.test_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.tmp_x)[idx]\n",
    "\n",
    "\"\"\"\n",
    "학습 최적화를 위해 스케줄러를 활용합니다.\n",
    "Pytorch 및 transformer의 스케줄러를 참고.\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py\n",
    "\"\"\"\n",
    "def get_constant_schedule(optimizer, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a constant learning rate.\n",
    "    \"\"\"\n",
    "    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "def get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a constant learning rate preceded by a warmup\n",
    "    period during which the learning rate increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "        return 1.0\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases linearly after\n",
    "    linearly increasing during a warmup period.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function between 0 and `pi * cycles` after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, num_training_steps, num_cycles=1.0, last_epoch=-1\n",
    "):\n",
    "    \"\"\" 학습률이 웜업 기간 이후 몇 번의 하드 리스타트를 하는 코사인 함수 값에 따라 감소하는\n",
    "    스케줄러를 만듭니다. 웜업 기간에는 학습률이 0과 1 사이에서 선형으로 증가합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        if progress >= 1.0:\n",
    "            return 0.0\n",
    "        return max(0.0, \\\n",
    "            0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd4caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vp": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "title_cell": "VisualPython",
   "title_sidebar": "VisualPython",
   "vpPosition": {
    "height": "calc(100% - 180px)",
    "right": "10px",
    "top": "110px",
    "width": "50%"
   },
   "vp_cell": false,
   "vp_section_display": true,
   "vp_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
