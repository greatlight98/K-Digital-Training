{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ResNet 이란?\n",
    " - 참고 : https://datascienceschool.net/view-notebook/958022040c544257aa7ba88643d6c032/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. torchvision.models.resnet\n",
    " - resnet(18, 34, 50, 101, 152)를 만들 수 있다.\n",
    " - 3(RGB) * 224 * 224 입력을 기준으로 만들도록 되어 있다.\n",
    " - input size가 다른 경우 Resnet을 적용하려면 어떻게 해야될지 배워보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BasicBlock\n",
    "### (1) BasicBlock 이란?\n",
    " - BasicBlock은 ResNet 18과 34를 위한 설계이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### (2) BasicBlock의 구조\n",
    "#### 1) Input인 x가 들어온다.\n",
    "#### 2) Input인 x가 Convolution layer(3x3, 64 -> 3x3, 64)들을 거쳐서 아웃풋 f(x)가 된다.\n",
    "#### 3) 원래의 Input 값인 x(Identity)가 f(x)와 더해진다.\n",
    "#### 4) relu를 통과시켜 output을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BottleNeck\n",
    "### (1) BottleNeck 이란?\n",
    " - BottleNeck은 ResNet 50, 101, 152를 위한 설계이다.\n",
    " \n",
    "<br>\n",
    "\n",
    "### (2) BottleNeck의 구조\n",
    "#### 1) Input인 x가 들어온다.\n",
    "#### 2) Input인 x가 Convolution layer(1x1, 64 -> 3x3, 64 -> 1x1, 256)들을 거쳐서 아웃풋 f(x)가 된다.\n",
    "#### 3) 원래의 Input 값인 x(Identity)가 f(x)와 더해진다.\n",
    "#### 4) relu를 통과시켜 output을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ResNet 네트워크 뜯어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "# 변수 선언\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "\n",
    "# 모델 url 선언\n",
    "model_urls = {\n",
    "  'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "  'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "  'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "  'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "  'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "# conv3x3 정의 : 3 * 3의 커널, 패딩 1로 정의\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "# conv1x1 정의 : 1 * 1의 커널, 패딩 0으로 정의\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "  \n",
    "# BasicBlock\n",
    "'''\n",
    " - BasicBlock은 ResNet 18과 34를 위한 설계이다.\n",
    " (1) Input인 x가 들어온다.\n",
    " (2) Input인 x가 Convolution layer(3x3, 64 -> 3x3, 64)들을 거쳐서 아웃풋 f(x)가 된다.\n",
    " (3) 원래의 Input 값인 x(Identity)가 f(x)와 더해진다.\n",
    " (4) relu를 통과시켜 output을 구한다.\n",
    "'''\n",
    "class BasicBlock(nn.Module):\n",
    "  expansion = 1\n",
    "  \n",
    "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "    super(BasicBlock, self).__init__()\n",
    "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.conv2 = conv3x3(planes, planes)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.downsample = downsample\n",
    "    self.stride = stride\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # identity\n",
    "    identity = x # Input인 x 값을 identity에 저장한다.\n",
    "    \n",
    "    # f(x)\n",
    "    out = self.conv1(x) # 3 * 3 convolution, stride는 파라미터 값\n",
    "    out = self.bn1(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.conv2(out) # 3 * 3 convolution, stride는 1\n",
    "    out = self.bn2(out)\n",
    "    \n",
    "    # downsample : f(x)는 convolution layer들을 거치기 때문에 크기가 작아질 수 있다. 그 경우 f(x)와 x의 크기 차이 때문에 f(x) + x가 불가능 하다. 이를 방지하기 위해 downsample을 하는 것이다.\n",
    "    if self.downsample is not None: # downsample이 None이 아니면\n",
    "      identity = self.downsample(x) # downsaple을 한다.\n",
    "    \n",
    "    # f(x) + x\n",
    "    out += idenity\n",
    "    \n",
    "    # relu\n",
    "    out = self.relu(out)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BottleNeck\n",
    "'''\n",
    " - BottleNeck은 ResNet 50, 101, 152를 위한 설계이다.\n",
    "(1) Input인 x가 들어온다.\n",
    "(2) Input인 x가 Convolution layer(1x1, 64 -> 3x3, 64 -> 1x1, 256)들을 거쳐서 아웃풋 f(x)가 된다.\n",
    "(3) 원래의 Input 값인 x(Identity)가 f(x)와 더해진다.\n",
    "(4) relu를 통과시켜 output을 구한다.\n",
    "'''\n",
    "class Bottleneck(nn.Module):\n",
    "  expansion = 4\n",
    "  \n",
    "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "    super(Bottleneck, self).__init__()\n",
    "    self.conv1 = conv1x1(inplanes, planes)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = conv3x3(planes, planes, stride)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "    self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.downsample = downsample\n",
    "    self.stride = stride\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # identity\n",
    "    identity = x\n",
    "    \n",
    "    # f(x)\n",
    "    out = self.conv1(x) # 1 * 1 convolution, stride는 1\n",
    "    out = self.bn1(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.conv2(out) # 3 * 3 convolution, stride는 파라미터 값\n",
    "    out = self.bn2(out)\n",
    "    out = self.relu(out)    \n",
    "    out = self.conv3(out) # 1 * 1 convolution, stride는 1\n",
    "    out = self.bn3(out)\n",
    "    \n",
    "    # downsample\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(x)\n",
    "    \n",
    "    # f(x) + x\n",
    "    out += identity\n",
    "    \n",
    "    # relu\n",
    "    out = self.relu(out)\n",
    "    \n",
    "    return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 네트워크 만들기\n",
    "class ResNet(nn.Module):\n",
    "  def __init__(self, block, layers, num_classes=1000, zero_init_residual=False): # 파라미터 값으로 block(BasicBlock or BottleNeck), class 수, zero_init_residual(True로도 해보기) 등 설정\n",
    "    super(ResNet, self).__init__()\n",
    "    \n",
    "    self.inplanes = 64\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) # input 3 * 224 * 224 -> output 64 * 112 * 112\n",
    "    \n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # input 64 * 112 * 112 -> output 64 * 56 * 56\n",
    "    \n",
    "    # ResNet 50 기준으로 레이어를 이해해 보자\n",
    "    self.layer1 = self._make_layer(block, 64, layers[0]) # layers[0] = 3\n",
    "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # layers[1] = 4\n",
    "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2) # layers[2] = 6\n",
    "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2) # layers[3] = 3\n",
    "    \n",
    "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "    \n",
    "    # weight initialize\n",
    "    for m in self.modules(): # 모듈의 값을 하나씩 가져와서\n",
    "      if isinstance(m, nn.Conv2d): # Conv이면\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "      elif isinstance(m, nn.BatchNrom2d): # BatchNorm이면\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    # zero_init_residual\n",
    "    if zero_init_residual: # zero_init_residual = True 일 때 실행 (성능이 0.2 ~ 0.3% 높아질 수 있다)\n",
    "      for m in self.modules():\n",
    "        if isinstance(m, Bottleneck):\n",
    "          nn.init.constant_(m.bn3.weight, 0)\n",
    "        elif isinstance(m, BasicBlock):\n",
    "          nn.init.constant_(m.bn2.weight, 0)\n",
    "  \n",
    "  # ResNet 50 기준으로 이해해 보자\n",
    "  def _make_layer(self, block, planes, blocks, stride=1): # BottleNeck, 64, 3 \n",
    "    downsample = None\n",
    "    \n",
    "    # downsample은 f(x)와 x의 크기를 맞추거나, 채널 사이즈를 맞추기 위해 사용한다.\n",
    "    if stride != 1 or self.inplanes != planes * block.expansion: # stride가 1이 아니거나, 'self.inplanes != planes * block.expansion(64 != 64 *4)' 라면, downsample을 실행하라.\n",
    "      downsample = nn.Sequential( # downsample 진행\n",
    "        conv1x1(self.inplanes, planes * block.expansion, stride), # conv1x1(64, 256, stride=1)\n",
    "        nn.BatchNorm2d(planes * block.expansion), # BatchNorm2d(256)\n",
    "      )\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(block(self.inplanes, planes, stride, downsample)) # layers.append(BottleNeck(64, 64, 1, downsample))\n",
    "        \n",
    "    self.inplanes = planes * block.expansion # self.inplanes = 256\n",
    "    \n",
    "    for _ in range(1, blocks): # for _ in range(1, 3)\n",
    "      layers.append(block(self.inplanes, planes))\n",
    "      \n",
    "    return nn.Sequential(*layers)\n",
    "    \n",
    "    '''\n",
    "    여기까지의 결과 ResNet 50의 첫번째 레이어에는 아래와 같이 쌓여있다는 걸 알 수 있다.\n",
    "    [\n",
    "      BottleNeck(64, 64, 1, downsample)\n",
    "      BottleNeck(256, 64)\n",
    "      BottleNeck(256, 64)\n",
    "    ]\n",
    "    \n",
    "    '''\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "    \n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "    \n",
    "    x = self.avgpool(x) # avgpool\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet18 정의\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "  model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) # BasicBlock 구조 이용. 2 * (2 + 2 + 2 + 2) + 1(conv1) + 1(fc1) = 18 => ResNet18\n",
    "  return model\n",
    "\n",
    "# resnet50 정의\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "  model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) # BottleNeck 구조 이용. 3 * (3 + 4 + 6 + 3) + 1(conv1) + 1(fc1) = 50 => ResNet50\n",
    "  return model\n",
    "\n",
    "# resnet152 정의\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "  model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) # BottleNeck 구조 이용. 3 * (3 + 8 + 36 + 3) + 1(conv1) + 1(fc1) = 152 => ResNet152\n",
    "  return model\n",
    "\n",
    "# resnet 사용 방법\n",
    "import torchvision.models.resnet as resnet # 라이브러리 로드\n",
    "res = resnet.resnet50() # 모델\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
